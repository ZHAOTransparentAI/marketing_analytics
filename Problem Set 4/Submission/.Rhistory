print(paste("Columns of the train data:", paste(colnames(df_train), collapse = ", ")))
# Because both datasets have the column "store_nbr" we can merge them
df_merged <- merge(df_stores, df_train, by = "store_nbr")
print(paste("Columns of the merged data:", paste(colnames(df_merged), collapse = ", ")))
# Lets examine how many rows we have in the merged dataset
print(paste("Number of rows in the merged dataset:", nrow(df_merged)))
# Also check how many unique stores we have in the data set
print(paste("Number of unique stores in the merged dataset:", length(unique(df_merged$store_nbr))))
# As we can see the store with the id 44 has the highest volume of sales
# Now we take a look at the variability in sales between the stores, for this we can look at the standard deviation. The higher the more variability
# Calculate the standard deviation of sales for each store
df_variability <- df_merged %>%
group_by(store_nbr) %>%
summarize(std_dev_sales = sd(sales, na.rm = TRUE))
# Sort to find the store with the highest variability
df_variability <- df_variability[order(-df_variability$std_dev_sales), ]
# Create a bar plot to visualize the variability
ggplot(df_variability, aes(x = store_nbr, y = std_dev_sales)) +
geom_bar(stat = "identity") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Variability in Sales per Store", x = "Store Number", y = "Standard Deviation of Sales")
# As we can see the store with the id 44 has the highest variability in sales
df_44 <- df_merged[df_merged$store_nbr == 44, ]
df_44
# Now we need to ensure that the data is properly formatted
# Convert 'date' column to Date type
df_44$date <- as.Date(df_44$date)
# Order the data by date
df_44 <- df_44[order(df_44$date), ]
# Checking the structure of the data
str(df_44)
# aggregate the sales
df_agg <- df_44 %>%
group_by(date) %>%
summarise(total_sales = sum(sales),
total_promotion = sum(onpromotion))
# Assuming your data frame is named 'df_agg' with columns 'date' and 'total_sales'
# Make sure the 'date' column is in Date format
library(dplyr)
# Step 1: Group data by year
df_agg <- df_agg %>%
mutate(Year = lubridate::year(date)) %>%
group_by(Year)
df_agg$daily_avg_year <- ave(df_agg$total_sales, df_agg$Year, FUN = mean)
# total_sales for yyyy-01-01 should has the value of daily_avg_year for that year
df_agg <- df_agg %>%
mutate(total_sales = ifelse(lubridate::month(date) == 1 & lubridate::day(date) == 1, daily_avg_year, total_sales))
# Assuming your data frame is named 'df_agg' with columns 'date' and 'total_sales'
# Make sure the 'date' column is in Date format
library(dplyr)
# Step 1: Group data by year
df_agg <- df_agg %>%
mutate(Year = lubridate::year(date)) %>%
group_by(Year)
# Step 2: Identify years that already contain 'yyyy-12-24' and 'yyyy-12-26'
years_with_extra_dates <- df_agg %>%
filter(format(date, "%m-%d") %in% c('12-24', '12-26')) %>%
distinct(Year)
# Step 3: Add 'yyyy-12-25' for every year that already contains 'yyyy-12-24' and 'yyyy-12-26'
df_agg_extra <- years_with_extra_dates %>%
summarise(date = as.Date(paste0(Year, "-12-25")))
# Step 4: Combine the original data with the additional lines
df_agg <- bind_rows(df_agg, df_agg_extra)
# Now 'df_agg' contains all original lines and additional lines for 'yyyy-12-25' for every year that already contains 'yyyy-12-24' and 'yyyy-12-26'
# daily_avg_year of yyyy-12-25 should have the same value of daily_avg_year as all other lines in that year
df_agg <- df_agg %>%
mutate(daily_avg_year = ifelse(lubridate::month(date) == 12 & lubridate::day(date) == 25, daily_avg_year[1], daily_avg_year))
df_agg <- df_agg %>%
mutate(total_sales = ifelse(lubridate::month(date) == 12 & lubridate::day(date) == 25, daily_avg_year, total_sales))
# sort df_agg by date
df_agg <- df_agg %>%
arrange(date)
# To properly analyze the data we need to consider the earthquake that happend in 2016
# We should exclude this period from our analysis as it is not representative of the normal sales
# First we need to identify the impact period of the earthquake
# Convert 'date' column to Date type if not already done
df_agg$date <- as.Date(df_agg$date)
# Filter data for the period from March to May 2016
df_earthquake_period <- subset(df_agg, date >= as.Date('2016-03-01') & date <= as.Date('2016-05-31'))
# Plotting sales data around the earthquake period
ggplot(df_earthquake_period, aes(x = date, y = total_sales)) +
geom_line() +
geom_vline(xintercept = as.Date('2016-04-16'), color="red", linetype="dashed") +
labs(title = 'Sales Data for Store 44 (March to May 2016)', x = 'Date', y = 'Sales') +
theme_minimal()
# Filter data for the period from March to May 2016
df_earthquake_period <- subset(df_agg, date >= as.Date('2016-01-01') & date <= as.Date('2016-12-31'))
# Plotting sales data around the earthquake period
ggplot(df_earthquake_period, aes(x = date, y = total_sales)) +
geom_line() +
geom_vline(xintercept = as.Date('2016-04-16'), color="red", linetype="dashed") +
labs(title = 'Sales Data for Store 44 (March to May 2016)', x = 'Date', y = 'Sales') +
theme_minimal()
# create a time series
sales_ts <- ts(df_agg$total_sales, start=c(2013,1), frequency=1)
options(warn = -1)
ggplot(df_agg, aes(x = date, y = sales_ts)) +
geom_line() +
geom_smooth(method = "gam",
color = quinary,
se = FALSE) +  # Add a linear trend line
theme_minimal() +
labs(title = "Total sales Data for Store 44", x = "Date", y = "Total Sales")
# To remove heteroskedasticity we can log the data
# because the data includes zero values we decided to use a log-plus-one transformation
log_sales_ts <- log(sales_ts)
#plot(log_sales_ts, main = "Log of Time Series")
#plot log_sales_ts and include a linear trend line
ggplot(df_agg, aes(x = date, y = log_sales_ts)) +
geom_line() +
geom_smooth(method = "gam", color = quinary, se = FALSE) +  # Add a linear trend line
theme_minimal() +
labs(title = "Log of Total sales Data for Store 44", x = "Date", y = "Log of Total Sales")
# The data needs to be stationary. To check if they are stationary we can use the Augmented Dickey-Fuller Test
# Perform the Augmented Dickey-Fuller Test
adf_test_daily <- adf.test(log_sales_ts, alternative = "stationary")
# Display the results
print(adf_test_daily)
# Running Kpss-test to check for stationarity
kpss_test_daily <- kpss.test(log_sales_ts, null = "Level")
print(kpss_test_daily)
# hypotheses of kpss test
# H0: The process is trend stationary
# H1: The process is not trend stationary
# Differencing the data
trdiff_log_sales_ts <- diff(log_sales_ts, lag = 1)
kpss_test_daily2 <- kpss.test(trdiff_log_sales_ts, null = "Level")
print(kpss_test_daily2)
plot(trdiff_log_sales_ts, main = "Differenced Log Sales Time Series")
# ACF plot of differenced time series
acf(trdiff_log_sales_ts, main = "ACF of Differenced Time Series")
# PACF plot of differenced time series
pacf(trdiff_log_sales_ts, main = "PACF of Differenced Time Series")
# This variable will be used in future steps
differencing_value = 1
# Differencing to remove weekly seasonality
wdiff_trdiff_log_sales_ts <- diff(trdiff_log_sales_ts, lag = 7)
# ACF plot of differenced time series
acf(wdiff_trdiff_log_sales_ts, main = "ACF of Twice Differenced Time Series")
# PACF plot of differenced time series
pacf(wdiff_trdiff_log_sales_ts, main = "PACF of Twice Differenced Time Series")
acf(wdiff_trdiff_log_sales_ts, lag.max = 366)
pacf(wdiff_trdiff_log_sales_ts, lag.max = 366)
acf(wdiff_trdiff_log_sales_ts, lag.max = 31)
pacf(wdiff_trdiff_log_sales_ts, lag.max = 31)
# Function to plot sum of squared residuals for different AR(p) models
plot_sar1_residuals <- function(time_series, min_p, max_p) {
# Initialize vectors to store p values and corresponding sum of squared residuals
p_values <- min_p:max_p
sum_of_squares <- numeric(length(p_values))
# Loop over the range of p values
for (i in seq_along(p_values)) {
# Fit the ARIMA model
model <- arima(time_series, order = c(p_values[i], 1, 0), seasonal = list(order = c(1, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(p_values, sum_of_squares, type = "b", col = primary,
xlab = "Order p of AR(p) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different AR(p) Models")
return(sum_of_squares)
}
plot_sma1_residuals <- function(time_series, min_q, max_q) {
# Initialize vectors to store q values and corresponding sum of squared residuals
q_values <- min_q:max_q
sum_of_squares <- numeric(length(q_values))
# Loop over the range of q values
for (i in seq_along(q_values)) {
# Fit the ARIMA model with p=0, d=0, and varying q
model <- arima(time_series, order = c(0, 1, q_values[i]), seasonal = list(order = c(1, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(q_values, sum_of_squares, type = "b", col = secondary,
xlab = "Order q of MA(q) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different MA(q) Models")
return(sum_of_squares)
}
# Function to plot sum of squared residuals for different AR(p) models
plot_sar0_residuals <- function(time_series, min_p, max_p) {
# Initialize vectors to store p values and corresponding sum of squared residuals
p_values <- min_p:max_p
sum_of_squares <- numeric(length(p_values))
# Loop over the range of p values
for (i in seq_along(p_values)) {
# Fit the ARIMA model
model <- arima(time_series, order = c(p_values[i], 1, 0), seasonal = list(order = c(0, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(p_values, sum_of_squares, type = "b", col = primary,
xlab = "Order p of AR(p) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different AR(p) Models")
return(sum_of_squares)
}
plot_sma0_residuals <- function(time_series, min_q, max_q) {
# Initialize vectors to store q values and corresponding sum of squared residuals
q_values <- min_q:max_q
sum_of_squares <- numeric(length(q_values))
# Loop over the range of q values
for (i in seq_along(q_values)) {
# Fit the ARIMA model with p=0, d=0, and varying q
model <- arima(time_series, order = c(0, 1, q_values[i]), seasonal = list(order = c(0, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(q_values, sum_of_squares, type = "b", col = secondary,
xlab = "Order q of MA(q) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different MA(q) Models")
return(sum_of_squares)
}
# Function to plot sum of squared residuals for different AR(p) models
plot_sar3_residuals <- function(time_series, min_p, max_p) {
# Initialize vectors to store p values and corresponding sum of squared residuals
p_values <- min_p:max_p
sum_of_squares <- numeric(length(p_values))
# Loop over the range of p values
for (i in seq_along(p_values)) {
# Fit the ARIMA model
model <- arima(time_series, order = c(p_values[i], 1, 0), seasonal = list(order = c(3, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(p_values, sum_of_squares, type = "b", col = primary,
xlab = "Order p of AR(p) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different AR(p) Models")
return(sum_of_squares)
}
plot_sma3_residuals <- function(time_series, min_q, max_q) {
# Initialize vectors to store q values and corresponding sum of squared residuals
q_values <- min_q:max_q
sum_of_squares <- numeric(length(q_values))
# Loop over the range of q values
for (i in seq_along(q_values)) {
# Fit the ARIMA model with p=0, d=0, and varying q
model <- arima(time_series, order = c(0, 1, q_values[i]), seasonal = list(order = c(3, 1, 1), period = 7))
# Calculate the sum of squared residuals
residuals <- residuals(model)
sum_of_squares[i] <- sum(residuals^2)
}
# Create a plot
plot(q_values, sum_of_squares, type = "b", col = secondary,
xlab = "Order q of MA(q) Model", ylab = "Sum of Squared Residuals",
main = "Sum of Squared Residuals for Different MA(q) Models")
return(sum_of_squares)
}
suppressWarnings(
sar1 <- plot_sar1_residuals(log_sales_ts, 0, 10)
)
suppressWarnings(
sma1 <- plot_sma1_residuals(log_sales_ts, 0, 10)
)
suppressWarnings(
sar0 <- plot_sar0_residuals(log_sales_ts, 0, 10)
)
suppressWarnings(
sma0 <- plot_sma0_residuals(log_sales_ts, 0, 10)
)
suppressWarnings(
sar3 <- plot_sar3_residuals(log_sales_ts, 0, 7)
)
suppressWarnings(
sma3 <- plot_sma3_residuals(log_sales_ts, 0, 10)
)
# Setting up the different models
model1 <- arima(log_sales_ts, order = c(4,1,1), seasonal = list(order = c(1,1,1), period = 7))
model2 <- arima(log_sales_ts, order = c(4,1,2), seasonal = list(order = c(1,1,1), period = 7))
model3 <- arima(log_sales_ts, order = c(2,1,1), seasonal = list(order = c(1,1,1), period = 7))
model4 <- arima(log_sales_ts, order = c(2,1,2), seasonal = list(order = c(1,1,1), period = 7))
model5 <- arima(log_sales_ts, order = c(4,1,1), seasonal = list(order = c(0,1,1), period = 7))
model6 <- arima(log_sales_ts, order = c(2,1,2), seasonal = list(order = c(0,1,1), period = 7))
model7 <- arima(log_sales_ts, order = c(4,1,2), seasonal = list(order = c(0,1,1), period = 7))
model8 <- arima(log_sales_ts, order = c(2,1,1), seasonal = list(order = c(0,1,1), period = 7))
model9 <- arima(log_sales_ts, order = c(2,1,2), seasonal = list(order = c(3,1,1), period = 7))
#model10 <- arima(log_sales_ts, order = c(2,1,1), seasonal = list(order = c(3,1,1), period = 7))
model11 <- arima(log_sales_ts, order = c(4,1,2), seasonal = list(order = c(3,1,1), period = 7))
# Comparing the AIC and BIC of the different models
AIC(model1, model2, model3, model4, model5, model6, model7, model8, model9, model11)
#RMSE of models
sqrt(mean(residuals(model1)^2))
sqrt(mean(residuals(model2)^2))
sqrt(mean(residuals(model3)^2))
sqrt(mean(residuals(model4)^2))
sqrt(mean(residuals(model5)^2))
sqrt(mean(residuals(model6)^2))
sqrt(mean(residuals(model7)^2))
sqrt(mean(residuals(model8)^2))
sqrt(mean(residuals(model9)^2))
sqrt(mean(residuals(model11)^2))
print("a")
#MAE of models
mean(abs(residuals(model1)))
mean(abs(residuals(model2)))
mean(abs(residuals(model3)))
mean(abs(residuals(model4)))
mean(abs(residuals(model5)))
mean(abs(residuals(model6)))
mean(abs(residuals(model7)))
mean(abs(residuals(model8)))
mean(abs(residuals(model9)))
mean(abs(residuals(model11)))
# show residuals of both models
tsdisplay(residuals(model1))
tsdisplay(residuals(model2))
tsdisplay(residuals(model3))
tsdisplay(residuals(model4))
tsdisplay(residuals(model5))
tsdisplay(residuals(model6))
tsdisplay(residuals(model7))
tsdisplay(residuals(model8))
tsdisplay(residuals(model9))
tsdisplay(residuals(model11))
# show me the density plot of the residuals of both models
plot(density(residuals(model1)))
plot(density(residuals(model2)))
plot(density(residuals(model3)))
plot(density(residuals(model4)))
plot(density(residuals(model5)))
plot(density(residuals(model6)))
plot(density(residuals(model7)))
plot(density(residuals(model8)))
plot(density(residuals(model9)))
plot(density(residuals(model11)))
forecasted_values <- forecast(model9, h = 15)
# Revert the log transformation
forecasted_values$mean <- exp(forecasted_values$mean)
forecasted_values$lower <- exp(forecasted_values$lower)
forecasted_values$upper <- exp(forecasted_values$upper)
# Create the date sequence
start_date <- as.Date("2017-08-15")
dates <- seq.Date(start_date, by = "day", length.out = 15)
# Create the table
forecast_table <- data.frame(
Date = dates,
Mean = forecasted_values$mean,
Lower = forecasted_values$lower[,1],  # assuming 80% confidence interval
Upper = forecasted_values$upper[,1]   # assuming 80% confidence interval
)
# Sum the forecasts
sum_mean <- sum(forecast_table$Mean)
sum_lower <- sum(forecast_table$Lower)
sum_upper <- sum(forecast_table$Upper)
print(paste("The aggregated mean forecast is: ", round(sum_mean)))
print(paste("The aggregated lower forecast is: ", round(sum_lower)))
print(paste("The aggregated upper forecast is: ", round(sum_upper)))
# enddate?
end_date = as.Date("2017-08-15")
# Create a sequence of dates for the forecast
forecast_dates <- seq(from = end_date + 1, by = "day", length.out = 15)
# Convert forecasted values and confidence intervals back to the original scale
#forecast_mean_original_scale <- exp(forecasted_values$mean)
#forecast_lower_original_scale <- exp(forecasted_values$lower[, "95%"])
#forecast_upper_original_scale <- exp(forecasted_values$upper[, "95%"])
# Plot the forecast on the original scale
plot(forecast_dates, forecasted_values$mean, type = "l", col = secondary,
ylim = range(forecasted_values$lower[, "95%"], forecasted_values$upper[, "95%"]),
main = "SARIMA(2,1,2)(3,1,1)7 Forecast for Store 44 Sales (Next 15 Days)", xlab = "Date", ylab = "Forecast Sales")
# Add ribbon for confidence intervals
polygon(c(forecast_dates, rev(forecast_dates)),
c(forecasted_values$lower[, "95%"], rev(forecasted_values$upper[, "95%"])),
col = "grey90", border = NA)
lines(forecast_dates, forecasted_values$mean, col = quinary)
# Vorbereitung Datenframe für Ölpreis und Verkaufszahlen
df_oil$date <- as.Date(df_oil$date)
df_agg$date <- as.Date(df_agg$date)
df_44_var <- merge(df_oil, df_agg, by = "date", all = TRUE)
# Umgang mit fehlenden Ölpreisen und 1. Januar
df_44_var <- df_44_var %>%
filter(date != as.Date("2013-01-01"))
# Ersetzen fehlende Werte durch den Wert davor
df_44_var$oil_price <- zoo::na.locf(df_44_var$dcoilwtico)
ggplot(df_44_var, aes(x = date, y = oil_price)) +
geom_line() +
labs(title = "Oil price over time", x = "date", y = "oil price")
suppressWarnings(
ggplot(df_44_var, aes(x = date, y = total_promotion)) +
geom_line() +
labs(title = "Promotion over time", x = "date", y = "promotions")
)
suppressWarnings(
ggplot(df_44_var, aes(x = date, y = total_sales)) +
geom_line() +
labs(title = "Sales over time", x = "date", y = "sales")
)
# Filter für Betrachtungszeitraum
df_44_var <- df_44_var %>% filter(date >= as.Date("2014-07-01"))
df_44_var <- df_44_var%>%
filter(date < as.Date("2017-08-16"))
# Funktion zum Ersetzen von Werten an bestimmten Tagen
replace_promotions_on_specific_dates <- function(data) {
library(dplyr)
data %>%
mutate(total_promotion = ifelse(
format(date, "%m-%d") %in% c("01-01", "12-25"),
mean(total_promotion, na.rm = TRUE),
total_promotion
))
}
# Anwenden der Funktion auf den vorhandenen Datenrahmen df_44_var
df_44_var <- replace_promotions_on_specific_dates(df_44_var)
# Running Kpss-test to check for stationarity
kpss_test_daily <- kpss.test(df_44_var$total_promotion, null = "Level")
print(kpss_test_daily)
# hypotheses of kpss test
# H0: The process is trend stationary
# H1: The process is not trend stationary
adf_test_daily <- adf.test(df_44_var$oil_price, alternative = "stationary")
# Display the results
print(adf_test_daily)
# Make the data stationary
df_44_var$total_promotion <- ifelse(df_44_var$total_promotion == 0, 0.1, df_44_var$total_promotion)
# Daten loggen
df_44_var$log_oil_price <- log(df_44_var$oil_price)
df_44_var$log_total_sales <- log(df_44_var$total_sales)
df_44_var$log_total_promotion <- log(df_44_var$total_promotion)
# Differencing
df_44_var$diff_log_oil_price <- c(NA, diff(df_44_var$log_oil_price))
df_44_var$diff_log_total_sales <- c(NA, diff(df_44_var$log_total_sales))
df_44_var$diff_log_total_promotion <- c(NA, diff(df_44_var$log_total_promotion))
df_44_var <- df_44_var %>% filter(date > as.Date("2014-07-01"))
ggplot(df_44_var, aes(x = date, y = diff_log_oil_price)) +
geom_line() +
labs(title = "Oil price over time", x = "date", y = "oil price")
ggplot(df_44_var, aes(x = date, y = diff_log_total_promotion)) +
geom_line() +
labs(title = "Promotion over time", x = "date", y = "promotions")
ggplot(df_44_var, aes(x = date, y = diff_log_total_sales)) +
geom_line() +
labs(title = "Sales over time", x = "date", y = "sales")
# Beispiel für die Auswahl der Lags basierend auf BIC
max_lags <- 31
bic_values <- numeric(max_lags)
for (i in 1:max_lags) {
var_model <- VAR(df_44_var[, c("diff_log_total_sales", "diff_log_oil_price" , "diff_log_total_promotion")], p = i)
bic_values[i] <- BIC(var_model)
}
best_lag_bic <- which.min(bic_values)
print(best_lag_bic)
# Durchführung VAR Modell
var_model <- VAR(df_44_var[, c("diff_log_total_sales", "diff_log_oil_price" , "diff_log_total_promotion")], p=7)
summary(var_model)
# Impulse Response Function erstellen
irf_result_oil_price <- irf(var_model, impulse= "diff_log_oil_price", n.ahead = 31)
plot(irf_result_oil_price)
irf_result_promotion <- irf(var_model, impulse= "diff_log_total_promotion", n.ahead = 31)
plot(irf_result_promotion)
irf_result_sales <- irf(var_model, impulse= "diff_log_total_sales", n.ahead = 31)
plot(irf_result_sales)
# Ölpreis der letzten Periode um 25% erhöhen
df_shock <- df_44_var
df_shock <- df_shock %>%
mutate(oil_price = ifelse(date == as.Date('2017-08-15'), oil_price * 1.25, oil_price)) %>%
mutate(oil_price = ifelse(date == as.Date('2017-08-14'), oil_price * 1.25, oil_price)) %>%
mutate(oil_price = ifelse(date == as.Date('2017-08-13'), oil_price * 1.25, oil_price))
df_shock$log_oil_price <- log(df_shock$oil_price)
df_shock$diff_log_oil_price_2 <- c(NA, diff(df_shock$log_oil_price))
df_shock_filtered <- df_shock[, c("date", "diff_log_total_sales" , "diff_log_oil_price_2", "diff_log_total_promotion")]
df_shock_filtered <- df_shock_filtered %>%
filter(date > as.Date("2014-07-03"))
var_model_shock <- VAR(df_shock_filtered[, c("diff_log_total_sales", "diff_log_oil_price_2" , "diff_log_total_promotion")], p=7)
# Forecast für 15 Perioden erstellen
forecast_result <- predict(var_model_shock , newdata = df_shock_filtered, n.ahead = 15)
df_forecast <- data.frame(forecast_result$fc$diff_log_total_sales)
# Inversion der Differenzierung durch kumulative Summe
df_forecast$log_total_sales <- cumsum(df_forecast$fcst) + tail(df_44_var$log_total_sales, 1)
df_forecast$log_total_sales_lower <- cumsum(df_forecast$lower) + tail(df_44_var$log_total_sales, 1)
df_forecast$log_total_sales_upper <- cumsum(df_forecast$upper) + tail(df_44_var$log_total_sales, 1)
df_forecast$log_total_sales_CI <- cumsum(df_forecast$CI) + tail(df_44_var$log_total_sales, 1)
# Ausgabe des rekonstruierten total_sales
df_forecast$reconstructed_total_sales_shock <- exp(df_forecast$log_total_sales)
df_forecast$reconstructed_total_sales_lower_shock <- exp(df_forecast$log_total_sales_lower)
df_forecast$reconstructed_total_sales_upper_shock <- exp(df_forecast$log_total_sales_upper)
df_forecast$reconstructed_total_sales_CI_shock <- exp(df_forecast$log_total_sales_CI)
# Forecast für 5 Perioden erstellen
forecast_result_2 <- predict(var_model, n.ahead = 15, newdata = df_44_var)
df_forecast_2 <- data.frame(forecast_result_2$fc$diff_log_total_sales)
# Inversion der Differenzierung durch kumulative Summe
df_forecast_2$log_total_sales <- cumsum(df_forecast_2$fcst) + tail(df_44_var$log_total_sales, 1)
df_forecast_2$log_total_sales_lower <- cumsum(df_forecast$lower) + tail(df_44_var$log_total_sales, 1)
df_forecast_2$log_total_sales_upper <- cumsum(df_forecast$upper) + tail(df_44_var$log_total_sales, 1)
df_forecast_2$log_total_sales_CI <- cumsum(df_forecast$CI) + tail(df_44_var$log_total_sales, 1)
# Ausgabe des rekonstruierten total_sales
df_forecast_2$reconstructed_total_sales <- exp(df_forecast_2$log_total_sales)
df_forecast_2$reconstructed_total_sales_lower <- exp(df_forecast_2$log_total_sales_lower)
df_forecast_2$reconstructed_total_sales_upper <- exp(df_forecast_2$log_total_sales_upper)
df_forecast_2$reconstructed_total_sales_CI <- exp(df_forecast_2$log_total_sales_CI)
# Erstellen eines Datenframes zum Vergleich des Forecasts
df_comparison <- data.frame(ForecastWithShock = df_forecast$reconstructed_total_sales_shock,  ForecastWithoutShock = df_forecast_2$reconstructed_total_sales)
start_date <- as.Date("2018-08-16")
df_comparison$date <- seq(start_date, by = "1 day", length.out = nrow(df_comparison))
print(df_comparison)
ggplot(data = df_comparison, aes(x = date)) +
geom_line(aes(y = ForecastWithShock, colour = "Forecast With Shock")) +
geom_line(aes(y = ForecastWithoutShock, colour = "Forecast Without Shock")) +
labs(title = "Forecast Comparison", x = "Date", y = "Value") +
theme_minimal()
# Differencing the data
trdiff_log_sales_ts <- diff(log_sales_ts, lag = 1)
trdiff_log_sales_ts
# Differencing the data
trdiff_log_sales_ts <- diff(log_sales_ts, lag = 1)
