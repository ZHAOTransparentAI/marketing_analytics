print(paste("RMSE:", evaluation_result$rmse))
print(paste("MAE:", evaluation_result$mae))
print(paste("R-squared:", evaluation_result$r_squared))
print(paste("MSE:", evaluation_result$mse))
print(paste("AIC:", evaluation_result$aic))
print(paste("BIC:", evaluation_result$bic))
print(paste("Adjusted R-squared:", evaluation_result$adjusted_r_squared))
return(list(model = model, evaluation_result = evaluation_result))
}
run_pipeline_cv <- function(data, show_output=TRUE, method="lm", random_seed = 1996, columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- data
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_cv(raw_data, method=method, random_seed=random_seed)
# print evaluation
if(show_output) {
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
}
# Calculate BIC and AIC and adjusted R-squared for final model
cv_result$finalModel$bic <- BIC(cv_result$finalModel)
cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
if(show_output) {
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
}
return(list(cv_result = cv_result))
}
run_pipeline_hyperparametertune <- function(method="glmnet", random_seed = 1996, metric = "MAE", formula = "G3 ~ .", tune_grid = expand.grid(alpha = 0:10/10, lambda = 0:10/10), columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- load_problemset1_data()
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_hyperparametertune(raw_data, method=method, metric = metric, formula = formula, tune_grid = tune_grid)
# print evaluation
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
# Calculate BIC and AIC and adjusted R-squared for final model
#cv_result$finalModel$bic <- BIC(cv_result$finalModel)
# Calculate BIC manually for final model
n <- nrow(raw_data)
k_bic <- sum(cv_result$finalModel$df > 0)  # Count non-zero coefficients as parameters
log_likelihood_bic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$bi <- -2 * log_likelihood_bic + k_bic * log(n)
# Calculate AIC manually for final model
k_aic <- k_bic  # For simplicity, assuming the same number of parameters for AIC
log_likelihood_aic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$aic <- 2 * k_aic - 2 * log_likelihood_aic
#cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
return(list(cv_result = cv_result))
}
# This section is excluded from the pdf
# It contains all relevant functions relevant for this section
# Additionally these functions are used to create the pipeline
# create holdout indizes
get_holdoutset_indizes <- function(data, seed, proportion = 0.2) {
# set seed
set.seed(seed)
# create holdout set
holdout_indices <- sample(1:nrow(data), size = proportion * nrow(data))
# return holdout set
return(holdout_indices)
}
train_model_cv <- function(train_data, method="lm", random_seed = 1996, formula="G3 ~ .") {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
set.seed(random_seed)
model <- train(formula,
data = train_data,
method = method,
trControl = trainControl(method = "cv", number = 5))
return(model)
}
# Function to train a linear regression model using the standard lm function
train_model_std <- function(train_data, formula="G3 ~ .") {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
model <- lm(formula,
data = train_data)
return(model)
}
# hyperparameter tune a linear regression model
train_model_hyperparametertune <- function(train_data, method="glmnet", metric = "MAE", formula = "G3 ~ .", tune_grid = expand.grid(alpha = 0:10/10, lambda = 0:10/10)) {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
model <- train(
formula,
data = train_data,
method = method,
trControl = trainControl(method = "cv", number = 5),
tuneGrid = tune_grid,
metric = metric
)
return(model)
}
# Function that removes columns from a data frame
remove_columns <- function(data, columns) {
data <- data[, !(colnames(data) %in% columns)]
return(data)
}
# Function that preprocesses the data
preprocess_data <- function(data, columns_to_keep = c(), remove_columns = c()) {
# remove G1, G2
data <- remove_columns(data, c("G1", "G2"))
if (length(columns_to_keep) > 0) {
data <- data[, columns_to_keep]
}
# remove the specified columns
else if (length(remove_columns) > 0) {
data <- remove_columns(data, remove_columns)
}
# Select the categorical values
char_columns <- sapply(data, is.character)
columns_cat <- names(data)[char_columns]
# Select the numerical values
columns_num <- setdiff(colnames(data), columns_cat)
columns_num
# One hot encode the categorical variables
#data <- one_hot_encode(data, columns_cat)
return(data)
}
# Function that creates a plot showing the p values
plot_p_values <- function(model, model_name = "Model", intercept_line = 0.05) {
# Tidy up the model results
tidy_lm <- na.omit(tidy(model))
# Create a bar plot of p-values
p_values_plot <- ggplot(tidy_lm, aes(x = term, y = p.value, fill = factor(p.value < intercept_line))) +
geom_bar(stat = "identity") +
geom_hline(yintercept = intercept_line, linetype = "dashed", color = "red") +
coord_flip() +
scale_fill_manual(values = c("grey", secondary), labels = c("Not Significant", "Significant")) +
theme_minimal() +
labs(title = paste("P-values for Coefficients of model:", model_name), x = "Coefficient", y = "P-value") +
theme(legend.position = "none")
print(p_values_plot)
}
# Function to evaluate a regression model
evaluate_regression_model <- function(model, test_data, target_variable = "G3") {
predictions <- predict(model, newdata = test_data)
# initialize evaluation result
evaluation_result <- list()
# Calculate R-squared
evaluation_result$r_squared <- cor(predictions, test_data[[target_variable]])^2
# Calculate Mean Absolute Error (MAE)
evaluation_result$mae <- mean(abs(predictions - test_data[[target_variable]]))
# Calculate Mean Squared Error (MSE)
evaluation_result$mse <- mean((predictions - test_data[[target_variable]])^2)
# Calculate Root Mean Squared Error (RMSE)
evaluation_result$rmse <- sqrt(evaluation_result$mse)
# Calculate BIC
evaluation_result$bic <- BIC(model)
# Calculate AIC
evaluation_result$aic <- AIC(model)
# Calculate Adjusted R-squared
evaluation_result$adjusted_r_squared <- summary(model)$adj.r.squared
return(evaluation_result)
}
# Function that plots the residuals of the model and the predicted vs. actual values.
plot_model_evaluation <- function(model, test_data, target_variable = "G3") {
predictions <- predict(model, newdata = test_data)
# Plot the residuals
plot(predictions, test_data[[target_variable]] - predictions, xlab = "Predictions", ylab = "Residuals")
# Plot the predicted vs. actual values
plot(predictions, test_data[[target_variable]], xlab = "Predictions", ylab = "Actual Values")
}
# Main pipeline function
run_pipeline_std <- function(random_seed = 1996, test_proportion = 0.3, columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- load_problemset1_data()
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# get indizes of holdout indizes
holdout_indices <- get_holdoutset_indizes(raw_data, seed = random_seed, proportion = test_proportion)
# Create the holdout set
holdout_set <- raw_data[holdout_indices, ]
# Create the training set
training_set <- raw_data[-holdout_indices, ]
# Train model
model <- train_model_std(training_set)
# Evaluate model
evaluation_result <- evaluate_regression_model(model, holdout_set)
print(paste("RMSE:", evaluation_result$rmse))
print(paste("MAE:", evaluation_result$mae))
print(paste("R-squared:", evaluation_result$r_squared))
print(paste("MSE:", evaluation_result$mse))
print(paste("AIC:", evaluation_result$aic))
print(paste("BIC:", evaluation_result$bic))
print(paste("Adjusted R-squared:", evaluation_result$adjusted_r_squared))
return(list(model = model, evaluation_result = evaluation_result))
}
run_pipeline_cv <- function(data, show_output=TRUE, method="lm", random_seed = 1996, columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- data
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_cv(raw_data, method=method, random_seed=random_seed)
# print evaluation
if(show_output) {
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
}
# Calculate BIC and AIC and adjusted R-squared for final model
cv_result$finalModel$bic <- BIC(cv_result$finalModel)
cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
if(show_output) {
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
}
return(list(cv_result = cv_result))
}
run_pipeline_hyperparametertune <- function(method="glmnet", random_seed = 1996, metric = "MAE", formula = "G3 ~ .", tune_grid = expand.grid(alpha = 0:10/10, lambda = 0:10/10), columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- load_problemset1_data()
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_hyperparametertune(raw_data, method=method, metric = metric, formula = formula, tune_grid = tune_grid)
# print evaluation
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
# Calculate BIC and AIC and adjusted R-squared for final model
#cv_result$finalModel$bic <- BIC(cv_result$finalModel)
# Calculate BIC manually for final model
n <- nrow(raw_data)
k_bic <- sum(cv_result$finalModel$df > 0)  # Count non-zero coefficients as parameters
log_likelihood_bic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$bi <- -2 * log_likelihood_bic + k_bic * log(n)
# Calculate AIC manually for final model
k_aic <- k_bic  # For simplicity, assuming the same number of parameters for AIC
log_likelihood_aic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$aic <- 2 * k_aic - 2 * log_likelihood_aic
#cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
return(list(cv_result = cv_result))
}
# This section is excluded from the pdf
# It contains all relevant functions relevant for this section
# Additionally these functions are used to create the pipeline
# create holdout indizes
get_holdoutset_indizes <- function(data, seed, proportion = 0.2) {
# set seed
set.seed(seed)
# create holdout set
holdout_indices <- sample(1:nrow(data), size = proportion * nrow(data))
# return holdout set
return(holdout_indices)
}
train_model_cv <- function(train_data, method="lm", random_seed = 1996, formula="G3 ~ .") {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
set.seed(random_seed)
model <- train(formula,
data = train_data,
method = method,
trControl = trainControl(method = "cv", number = 5))
return(model)
}
# Function to train a linear regression model using the standard lm function
train_model_std <- function(train_data, formula="G3 ~ .") {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
model <- lm(formula,
data = train_data)
return(model)
}
# hyperparameter tune a linear regression model
train_model_hyperparametertune <- function(train_data, method="glmnet", metric = "MAE", formula = "G3 ~ .", tune_grid = expand.grid(alpha = 0:10/10, lambda = 0:10/10)) {
# Check if the formula is a string and convert it to a formula object
if (is.character(formula)) {
formula <- as.formula(formula)
}
model <- train(
formula,
data = train_data,
method = method,
trControl = trainControl(method = "cv", number = 5),
tuneGrid = tune_grid,
metric = metric
)
return(model)
}
# Function that removes columns from a data frame
remove_columns <- function(data, columns) {
data <- data[, !(colnames(data) %in% columns)]
return(data)
}
# Function that preprocesses the data
preprocess_data <- function(data, columns_to_keep = c(), remove_columns = c()) {
# remove G1, G2
data <- remove_columns(data, c("G1", "G2"))
if (length(columns_to_keep) > 0) {
data <- data[, columns_to_keep]
}
# remove the specified columns
else if (length(remove_columns) > 0) {
data <- remove_columns(data, remove_columns)
}
# Select the categorical values
char_columns <- sapply(data, is.character)
columns_cat <- names(data)[char_columns]
# Select the numerical values
columns_num <- setdiff(colnames(data), columns_cat)
columns_num
# One hot encode the categorical variables
#data <- one_hot_encode(data, columns_cat)
return(data)
}
# Function that creates a plot showing the p values
plot_p_values <- function(model, model_name = "Model", intercept_line = 0.05) {
# Tidy up the model results
tidy_lm <- na.omit(tidy(model))
# Create a bar plot of p-values
p_values_plot <- ggplot(tidy_lm, aes(x = term, y = p.value, fill = factor(p.value < intercept_line))) +
geom_bar(stat = "identity") +
geom_hline(yintercept = intercept_line, linetype = "dashed", color = "red") +
coord_flip() +
scale_fill_manual(values = c("grey", secondary), labels = c("Not Significant", "Significant")) +
theme_minimal() +
labs(title = paste("P-values for Coefficients of model:", model_name), x = "Coefficient", y = "P-value") +
theme(legend.position = "none")
print(p_values_plot)
}
# Function to evaluate a regression model
evaluate_regression_model <- function(model, test_data, target_variable = "G3") {
predictions <- predict(model, newdata = test_data)
# initialize evaluation result
evaluation_result <- list()
# Calculate R-squared
evaluation_result$r_squared <- cor(predictions, test_data[[target_variable]])^2
# Calculate Mean Absolute Error (MAE)
evaluation_result$mae <- mean(abs(predictions - test_data[[target_variable]]))
# Calculate Mean Squared Error (MSE)
evaluation_result$mse <- mean((predictions - test_data[[target_variable]])^2)
# Calculate Root Mean Squared Error (RMSE)
evaluation_result$rmse <- sqrt(evaluation_result$mse)
# Calculate BIC
evaluation_result$bic <- BIC(model)
# Calculate AIC
evaluation_result$aic <- AIC(model)
# Calculate Adjusted R-squared
evaluation_result$adjusted_r_squared <- summary(model)$adj.r.squared
return(evaluation_result)
}
# Function that plots the residuals of the model and the predicted vs. actual values.
plot_model_evaluation <- function(model, test_data, target_variable = "G3") {
predictions <- predict(model, newdata = test_data)
# Plot the residuals
plot(predictions, test_data[[target_variable]] - predictions, xlab = "Predictions", ylab = "Residuals")
# Plot the predicted vs. actual values
plot(predictions, test_data[[target_variable]], xlab = "Predictions", ylab = "Actual Values")
}
# Main pipeline function
run_pipeline_std <- function(random_seed = 1996, test_proportion = 0.3, columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- load_problemset1_data()
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# get indizes of holdout indizes
holdout_indices <- get_holdoutset_indizes(raw_data, seed = random_seed, proportion = test_proportion)
# Create the holdout set
holdout_set <- raw_data[holdout_indices, ]
# Create the training set
training_set <- raw_data[-holdout_indices, ]
# Train model
model <- train_model_std(training_set)
# Evaluate model
evaluation_result <- evaluate_regression_model(model, holdout_set)
print(paste("RMSE:", evaluation_result$rmse))
print(paste("MAE:", evaluation_result$mae))
print(paste("R-squared:", evaluation_result$r_squared))
print(paste("MSE:", evaluation_result$mse))
print(paste("AIC:", evaluation_result$aic))
print(paste("BIC:", evaluation_result$bic))
print(paste("Adjusted R-squared:", evaluation_result$adjusted_r_squared))
return(list(model = model, evaluation_result = evaluation_result))
}
run_pipeline_cv <- function(data, show_output=TRUE, method="lm", random_seed = 1996, columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- data
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_cv(raw_data, method=method, random_seed=random_seed)
# print evaluation
if(show_output) {
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
}
# Calculate BIC and AIC and adjusted R-squared for final model
cv_result$finalModel$bic <- BIC(cv_result$finalModel)
cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
if(show_output) {
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
}
return(list(cv_result = cv_result))
}
run_pipeline_hyperparametertune <- function(method="glmnet", random_seed = 1996, metric = "MAE", formula = "G3 ~ .", tune_grid = expand.grid(alpha = 0:10/10, lambda = 0:10/10), columns_to_keep = c(), remove_columns = c()) {
# Load data
raw_data <- load_problemset1_data()
# preprocess the data
raw_data <- preprocess_data(raw_data, columns_to_keep = columns_to_keep, remove_columns = remove_columns)
# Train model
cv_result <- train_model_hyperparametertune(raw_data, method=method, metric = metric, formula = formula, tune_grid = tune_grid)
# print evaluation
cat("RMSE:", mean(cv_result$results$RMSE), "\n")
cat("MAE:", mean(cv_result$results$MAE), "\n")
cat("R-squared:", mean(cv_result$results$Rsquared), "\n")
cat("MSE:", mean(cv_result$results$RMSE^2), "\n")
# Calculate BIC and AIC and adjusted R-squared for final model
#cv_result$finalModel$bic <- BIC(cv_result$finalModel)
# Calculate BIC manually for final model
n <- nrow(raw_data)
k_bic <- sum(cv_result$finalModel$df > 0)  # Count non-zero coefficients as parameters
log_likelihood_bic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$bi <- -2 * log_likelihood_bic + k_bic * log(n)
# Calculate AIC manually for final model
k_aic <- k_bic  # For simplicity, assuming the same number of parameters for AIC
log_likelihood_aic <- -0.5 * sum((cv_result$finalModel$residuals)^2)  # Assuming Gaussian errors
cv_result$finalModel$aic <- 2 * k_aic - 2 * log_likelihood_aic
#cv_result$finalModel$aic <- AIC(cv_result$finalModel)
cv_result$finalModel$adjr2 <- summary(cv_result$finalModel)$adj.r.squared
cat("BIC:", cv_result$finalModel$bic, "\n")
cat("AIC:", cv_result$finalModel$aic, "\n")
cat("Adjusted R-squared:", mean(cv_result$finalModel$adjr2), "\n")
return(list(cv_result = cv_result))
}
# prepare data frame that contains the results
# with column names: model_name, rmse, mae, r_squared, mse, aic, bic, adjusted_r_squared
# This data frame will be used to store all relevant evaluations from the models
# that are following.
results <- data.frame(
model_name = character(),
rmse = numeric(),
mae = numeric(),
r_squared = numeric(),
mse = numeric(),
aic = numeric(),
bic = numeric(),
adjusted_r_squared = numeric()
)
# function that inputs data into the results data frame without
# changing the names of the columns
add_result <- function(df, input) {
df <- rbind(df, setNames(as.data.frame(t(input)), colnames(df)))
return(df)
}
# run a pipeline including all columns but G1 and G2
lm.full <- run_pipeline_cv(data=load_problemset1_data(), random_seed = 1996)
# run a pipeline including all columns but G1 and G2
lm.full <- run_pipeline_cv(data=load_problemset1_data(), random_seed = 1996)
# run a pipeline including all columns but G1 and G2
lm.full <- run_pipeline_cv(data=load_problemset1_data(), random_seed = 1996)
# prepare data frame that contains the results
# with column names: model_name, rmse, mae, r_squared, mse, aic, bic, adjusted_r_squared
# This data frame will be used to store all relevant evaluations from the models
# that are following.
results <- data.frame(
model_name = character(),
rmse = numeric(),
mae = numeric(),
r_squared = numeric(),
mse = numeric(),
aic = numeric(),
bic = numeric(),
adjusted_r_squared = numeric()
)
# function that inputs data into the results data frame without
# changing the names of the columns
add_result <- function(df, input) {
df <- rbind(df, setNames(as.data.frame(t(input)), colnames(df)))
return(df)
}
# run a pipeline including all columns but G1 and G2
lm.full <- run_pipeline_cv(data=load_problemset1_data(), random_seed = 1996)
